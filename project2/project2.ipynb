{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Helsinki, Master's Programme in Data Science  \n",
    "DATA20019 Trustworthy Machine Learning, Autumn 2021  \n",
    "Antti Honkela and Ossi Räisä  \n",
    "\n",
    "# Project 2: Real-life privacy-preserving machine learning\n",
    "\n",
    "Deadline for returning the solutions: 28 November 23:55.\n",
    "\n",
    "## General instructions (IMPORTANT!)\n",
    "\n",
    "1. This is an individual project. You can discuss the solutions with other students, but everyone needs to write their own code and answers.\n",
    "2. Please return your solutions as a notebook. When returning your solutions, please leave all output in the notebook.\n",
    "3. When returning your solutions, please make sure the notebook can be run cleanly using \"Cell\" / \"Run All\".\n",
    "4. Please make sure there are no dependencies between solutions to different problems.\n",
    "5. Please make sure that your notebook will not depend on any local files.\n",
    "6. Please make sure that the solutions for each problem in your notebook will produce the same results when run multiple times, i.e. remember to seed any random number generators you use (`numpy.random.seed()`, `torch.manual_seed()`!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1: Differentially private logistic regression with DP-SGD\n",
    "\n",
    "The Opacus (https://opacus.ai/) library provides implementations of many differentially private optimisation algorithms for deep learning and other models for the PyTorch (https://pytorch.org/) library. In order to perform these exercises, you will need to install Opacus, PyTorch and their dependencies according to instructions given on the websites. If you have not used PyTorch before, https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html is a good tutorial on the basics.\n",
    "\n",
    "In order to study Opacus, we will use logistic regression on the UCI Adult data set (https://archive.ics.uci.edu/ml/datasets/Adult). (The data set is a standard benchmark data set that is available in various packages - feel free to use one of those.)  \n",
    "\n",
    "A simple example implementation of the model is available at (https://www.cs.helsinki.fi/u/oraisa/tml/dp_log_reg_opacus.py).\n",
    "The code has been adapted from tutorials provided with Opacus.\n",
    "\n",
    "The definition of the logistic regression model binary classification is itself very straightforward in PyTorch, simply using a single fully connected linear layer with cross entropy loss:\n",
    "```{python}\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        # Define logistic regression using torch.nn layer and loss function\n",
    "        self.linear = nn.Linear(dim, 1, bias=bias)\n",
    "        self.loss = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "```\n",
    "Training with privacy is done using the `PrivacyEngine` class, which is simply attached to \n",
    "any supported PyTorch optimiser.\n",
    "```{python}\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    sample_rate=sample_rate,\n",
    "    alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 200)),\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    max_grad_norm=clip_bound,\n",
    "    secure_rng=False, # Should set to True for production use\n",
    ")\n",
    "# Fix randomness, will throw an error with secure_rng=True\n",
    "privacy_engine._set_seed(random_seed) \n",
    "privacy_engine.attach(optimizer)\n",
    "```\n",
    "Data subsampling is done by accessing the data through a `DataLoader` object that is configured with an Opacus-specific batch sampler.\n",
    "```{python}\n",
    "train_loader = DataLoader(\n",
    "    train_tensor,\n",
    "    batch_sampler=UniformWithReplacementSampler(\n",
    "        num_samples=len(train_tensor),\n",
    "        sample_rate=sample_rate,\n",
    "        # Should use a cryptographically secure PRNG in production\n",
    "        generator=torch.Generator().manual_seed(random_seed), \n",
    "    ),\n",
    ")\n",
    "```\n",
    "The `secure_rng` and `generator` parameters of `PrivacyEngine` and `UniformWithReplacementSampler` define the random number generators used for noise and minibatch sampling. In this exercise, we use the default RNGs of PyTorch, and manually seed them for reproducibility, but true DP guarantees require using a cryptographically secure RNG.\n",
    "\n",
    "The `alphas` parameter of `PrivacyEngine` is related to Rényi differential privacy (RDP), which Opacus uses internally to compute privacy bounds. RDP is an alternative definition of DP, where privacy bounds are parameterised by an $(\\alpha, \\epsilon)$-pair. Opacus computes RDP bounds for the given $\\alpha$ values, converts each RDP bound to an $(\\epsilon, \\delta)$-bound with $\\delta$ given, and returns the smallest $\\epsilon$, along with the corresponding $\\alpha$.\n",
    "This means that if the given list of $\\alpha$ values does not contain the optimal $\\alpha$, the returned $\\epsilon$ values are too large. As a simple heuristic, if the returned $\\alpha$ value is one of the bounds of the given list of $\\alpha$ values, you should expand the list.\n",
    "\n",
    "The rest of the example provides supporting architecture. The `generate_data` function generates a small test dataset, and `create_data_loaders` creates `DataLoader` objects for training and test data that can be passed to the `train` and `test` functions. Key parameters of the algorithm are defined at the end of the file and given as parameters to the `train` function. These include:\n",
    "```{python}\n",
    "# Learning rate for training\n",
    "learning_rate = 0.05\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 2\n",
    "# Clipping norm\n",
    "clip_bound = 1\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.03\n",
    "# Number of epochs\n",
    "num_epochs = 2\n",
    "```\n",
    "\n",
    "`learning_rate` is the initial learning rate for the SGD optimiser. Larger value means faster learning but can cause instability.  \n",
    "`noise_multiplier` controls the amount of noise added in DP-SGD: higher value means more noise. The value is defined relative to the gradient clip bound.  \n",
    "`clip_bound` is the maximum norm at which per-example gradients are clipped. Smaller values mean less noise with the same level of privacy, but too small values can bias the results and make learning impossible.  \n",
    "`sample_rate` is the fraction of full data used for each minibatch, which impacts privacy via amplification from subsampling. While a smaller sample rate increases privacy for a single update, it also increases the number of updates in a single epoch, which reduces privacy. The latter effect tends to be stronger than the first, so increasing sample rate increases privacy, but having too few updates limits the amount of learning.  \n",
    "`num_epochs` controls the length of training as a number of passes over the entire data.\n",
    "\n",
    "i. Test how these parameters (clip bound, sample rate, noise multiplier, learning rate and number of epochs) affect the accuracy of the classifier and its privacy. (You can use the test dataset from `generate_test_data` or the Adult dataset.)\n",
    "\n",
    "ii. How accurate classifier can you build to predict if an individual has an income of at most 50k with the Adult dataset, using DP with $\\epsilon=1, \\delta = 10^{-5}$? Report your accuracy on a separate test set not used in learning. (Opacus does not offer an easy way to set the parameters to get $\\epsilon = 1$, so you must tweak the parameters such that $\\epsilon \\leq 1$. You can use the function `check_privacy` to evaluate the privacy level at given parameters without actually running the optimisation.)\n",
    "\n",
    "Hint: the data set includes many categorical variables. In order to use these, you will need to use a one-hot encoding with $n-1$ variables used to denote $n$ values so that $k$th value is represented by value 1 in $k-1$st variable and zeros otherwise. You do not need to use all of the variables for DP training, as a large number of variables increases the noise DP-SGD has to add, while some of the variables may not be useful for the prediction task.\n",
    "\n",
    "Note: as noted at the lecture, testing several hyperparameters and choosing the best has an impact on the privacy guarantees.\n",
    "\n",
    "Note: Unlike what the name suggests, `UniformWithReplacementSampler` samples a minibatch by selecting each element independently with the probability `sample_rate`. This implies that different minibatches will be of different sizes. This form of sampling works well with add/remove neighbourhoods that are commonly used in DP deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question I used the random data generated by `generate_random_data()` function. I tested $\\epsilon$ and accuracy on different parameters:\n",
    "\n",
    "- learning_rate = [0.01, 0.05, 0.1, 0.5]\n",
    "- noise_multiplier = [1, 1.5, 2, 2.5]\n",
    "- clip_bound = [0.5, 1, 1.5, 2]\n",
    "- sample_rate = [0.01, 0.03, 0.1, 0.2]\n",
    "\n",
    "Then, I created a plot for each hyperparameter, relating accuracy score and $\\epsilon$ on the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparatory code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch opacus catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import opacus\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.linear = nn.Linear(dim, 1, bias=bias)\n",
    "        self.loss = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x).view(-1)\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss(out, y)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss(out, y)\n",
    "        preds = out > 0\n",
    "        corrects = torch.tensor(torch.sum(preds == y).item())\n",
    "        return loss, corrects\n",
    "\n",
    "def train(model, train_loader, opt_func, learning_rate, num_epochs, sample_rate, noise_multiplier, clip_bound, delta, random_seed=474237, verbose=False):\n",
    "    optimizer = opt_func(model.parameters(), learning_rate)\n",
    "    privacy_engine = opacus.PrivacyEngine(\n",
    "        model,\n",
    "        sample_rate=sample_rate,\n",
    "        alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 200)),\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=clip_bound,\n",
    "        secure_rng=False,\n",
    "    )\n",
    "    privacy_engine._set_seed(random_seed)\n",
    "    privacy_engine.attach(optimizer)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.train_step(batch)\n",
    "            loss.backward()\n",
    "            losses.append(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Epoch {}, loss = {}\".format(epoch + 1, torch.sum(loss) / len(train_loader)))\n",
    "\n",
    "    epsilon, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "    return epsilon, alpha\n",
    "\n",
    "def test(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        total_size = 0\n",
    "        for batch in test_loader:\n",
    "            total_size += len(batch[1])\n",
    "            loss, corrects = model.test_step(batch)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(corrects)\n",
    "\n",
    "        average_loss = np.array(loss).sum() / total_size\n",
    "        total_accuracy = np.array(accuracies).sum() / total_size\n",
    "        return average_loss, total_accuracy\n",
    "\n",
    "def create_data_loaders(train_tensor, test_tensor, sample_rate, random_seed=4732842):\n",
    "    train_loader = DataLoader(\n",
    "        train_tensor,\n",
    "        batch_sampler=UniformWithReplacementSampler(\n",
    "            num_samples=len(train_tensor),\n",
    "            sample_rate=sample_rate,\n",
    "            generator=torch.Generator().manual_seed(random_seed),\n",
    "        ),\n",
    "    )\n",
    "    test_loader = DataLoader(test_tensor, 64)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def check_privacy(sample_rate, noise_multiplier, num_epochs, delta = 1e-5,\n",
    "                  alphas = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 200))):\n",
    "    rdp = opacus.privacy_analysis.compute_rdp(sample_rate, noise_multiplier, int(1 / sample_rate) * num_epochs, alphas)\n",
    "    epsilon, opt_order = opacus.privacy_analysis.get_privacy_spent(alphas, rdp, delta)\n",
    "    return epsilon, opt_order\n",
    "\n",
    "def generate_random_data():\n",
    "    np.random.seed(4242)\n",
    "\n",
    "    n_train = 2000\n",
    "    n_test = 2000\n",
    "    input_dim = 5\n",
    "\n",
    "    N = n_train + n_test\n",
    "    X0 = np.random.randn(N, input_dim)\n",
    "    temp = X0 @ np.random.randn(input_dim, 1) + np.random.randn(N, 1)\n",
    "    Y0 = np.round(1/(1+np.exp(-temp)))\n",
    "\n",
    "    train_X = X0[0:n_train, :]\n",
    "    test_X = X0[n_train:N, :]\n",
    "    train_Y = Y0[0:n_train, 0]\n",
    "    test_Y = Y0[n_train:N, 0]\n",
    "    train_X = np.array(train_X, dtype=np.float32)\n",
    "    test_X = np.array(test_X, dtype=np.float32)\n",
    "    train_Y = np.array(train_Y, dtype=np.int32)\n",
    "    test_Y = np.array(test_Y, dtype=np.int32)\n",
    "\n",
    "    train_tensor = TensorDataset(torch.tensor(train_X), torch.tensor(train_Y, dtype=torch.float32))\n",
    "    test_tensor = TensorDataset(torch.tensor(test_X), torch.tensor(test_Y, dtype=torch.float32))\n",
    "\n",
    "    return train_tensor, test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actual code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTION\n",
    "torch.manual_seed(472368)\n",
    "np.random.seed(0)\n",
    "train_tensor, test_tensor = generate_random_data()\n",
    "input_dim = train_tensor[0][0].size(dim=0)\n",
    "\n",
    "delta = 1e-5\n",
    "\n",
    "# Stock values\n",
    "learning_rate = 0.05\n",
    "noise_multiplier = 2\n",
    "clip_bound = 1\n",
    "sample_rate = 0.03\n",
    "num_epochs = 2\n",
    "\n",
    "# Experiments\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.5]\n",
    "noise_multipliers = [1, 1.5, 2, 2.5]\n",
    "clip_bounds = [0.5, 1, 1.5, 2]\n",
    "sample_rates = [0.01, 0.03, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test learning rates\n",
    "for lr in learning_rates:\n",
    "    x = []\n",
    "    y = []\n",
    "    train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "    for epoch in range(1,20):\n",
    "        model = LogisticRegression(input_dim)\n",
    "        epsilon, alpha = train(\n",
    "            model, train_loader, torch.optim.SGD, lr, epoch, sample_rate,\n",
    "            noise_multiplier, clip_bound, delta\n",
    "        )\n",
    "        x.append(epsilon)\n",
    "        _, total_accuracy = test(model, test_loader)\n",
    "        y.append(total_accuracy)\n",
    "    plt.plot(x, y, label=lr)\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost.datasets import adult\n",
    "\n",
    "# prepare adult dataset\n",
    "train_set, test_set = adult()\n",
    "data = train_set.append(test_set, ignore_index=False)\n",
    "data = data.reset_index()\n",
    "del data['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2: DP Image Classification\n",
    "\n",
    "Using the above code as a basis, build a DP image classifier for the MNIST dataset. The dataset consists of 28x28 images of handwritten digits (0-9), and the task is to classify which digit each image represents. \n",
    "\n",
    "A simple implementation of a convolutional neural network for image classification is available (https://www.cs.helsinki.fi/u/oraisa/tml/pytorch_mnist.py), based on the Opacus MNIST tutorial. The example requires the Torchvision (https://pytorch.org/vision/stable/index.html) library, which you should install. The example contains the `SampleConvNet` class that can be used with the `train` and `test` functions from the last exercise, as well as the `get_mnist_dataset` function that loads the MNIST dataset. The dataset is downloaded the first time the function is called and is placed in a directory called `mnist`. By default, the function only loads half of the training data (30 000 images) to save computation time, but the number of training images can be given as a parameter.\n",
    "\n",
    "i. Train the model using the hyperparameters given below and compute accuracy on test data.\n",
    "```{python}\n",
    "clip_bound = 1.5\n",
    "learning_rate = 0.25\n",
    "sample_rate = 0.004\n",
    "noise_multiplier = 1.3\n",
    "num_epochs = 5\n",
    "```\n",
    "The training may take a minute or two. You can pass `verbose=True` to the `train` function to get updates for every completed epoch.\n",
    "You should get $\\epsilon \\approx 0.62$ with $\\delta = 10^{-5}$ and an accuracy of 80-90%.\n",
    "\n",
    "ii. Test the impact of the different DP parameters with the aim of creating maximally accurate classifier, given privacy bounds $\\epsilon = 1$, $\\delta = 10^{-5}$. You can also try changing the optimiser.\n",
    "\n",
    "Note: Running the learning with many parameters can take a long time, so you should plan your work to keep the number of runs reasonable. The grading of part ii is based on obtaining an overall picture of the impact of key parameters, not the absolute accuracy obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Your own problem in privacy-preserving machine learning\n",
    "\n",
    "State and solve your own problem related to privacy-preserving machine learning.\n",
    "\n",
    "You can use code available online, as long as you cite the source.\n",
    "\n",
    "You can for example try reproducing the results of some interesting paper using their data or your own data, try out some of the privacy attacks, or simply try the above examples using more complex models and/or on different data sets.\n",
    "\n",
    "If your problem is based on some previous problem, it should extend it in a non-trivial manner (not just running exact same code with new parameters or data).\n",
    "\n",
    "The evaluation of the project will take the difficulty of your chosen problem into account.\n",
    "\n",
    "This task is worth as much as two regular problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
